name: Build and Deploy to EKS

on:
  push:
    branches: [ "main" ]
    paths:
      - 'app/**'
      - 'docker/**'
      - 'k8s/**'
  pull_request:
    branches: [ "main" ]
    paths:
      - 'app/**'
      - 'docker/**'
      - 'k8s/**'
  workflow_dispatch:

permissions:
  contents: write
  pull-requests: write
  id-token: write

env:
  AWS_REGION: ap-south-1
  EKS_CLUSTER_NAME: flask-eks-cluster
  ECR_REPOSITORY: flask-microservice-app

jobs:
  build:
    name: Build and Push Docker Image
    runs-on: ubuntu-latest
    outputs:
      image: ${{ steps.image.outputs.image }}

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        role-to-assume: ${{ vars.AWS_ROLE_ARN }}
        aws-region: ${{ env.AWS_REGION }}

    - name: Login to Amazon ECR
      id: login-ecr
      uses: aws-actions/amazon-ecr-login@v2

    - name: Build, tag, and push image to Amazon ECR
      id: image
      env:
        ECR_REGISTRY: ${{ steps.login-ecr.outputs.registry }}
        IMAGE_TAG: ${{ github.sha }}
      run: |
        # Build a docker container and push it to ECR
        docker build -t $ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG -f docker/Dockerfile .
        docker push $ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG
        
        # Also tag as latest
        docker tag $ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG $ECR_REGISTRY/$ECR_REPOSITORY:latest
        docker push $ECR_REGISTRY/$ECR_REPOSITORY:latest
        
        echo "image=$ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG" >> $GITHUB_OUTPUT

  deploy:
    name: Deploy to EKS
    runs-on: ubuntu-latest
    needs: build
    if: github.ref == 'refs/heads/main'

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        role-to-assume: ${{ vars.AWS_ROLE_ARN }}
        aws-region: ${{ env.AWS_REGION }}

    - name: Update kube config
      run: aws eks update-kubeconfig --name $EKS_CLUSTER_NAME --region $AWS_REGION

    - name: Install eksctl
      run: |
        curl --silent --location "https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz" | tar xz -C /tmp
        sudo mv /tmp/eksctl /usr/local/bin

    - name: Install Helm
      run: |
        curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash

    - name: Setup OIDC Provider and Install AWS Load Balancer Controller
      run: |
        # Get account ID
        ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
        
        echo "Setting up OIDC provider for cluster..."
        # Associate OIDC provider with cluster (required for IAM service accounts)
        eksctl utils associate-iam-oidc-provider --region=$AWS_REGION --cluster=$EKS_CLUSTER_NAME --approve || echo "OIDC provider already exists"
        
        # Check if AWS Load Balancer Controller is already installed and working
        if kubectl get deployment -n kube-system aws-load-balancer-controller > /dev/null 2>&1; then
          echo "AWS Load Balancer Controller exists, checking if it's ready..."
          if kubectl wait --for=condition=available deployment/aws-load-balancer-controller -n kube-system --timeout=60s; then
            echo "AWS Load Balancer Controller is ready"
          else
            echo "AWS Load Balancer Controller exists but not ready, reinstalling..."
            # Clean up existing installation
            helm uninstall aws-load-balancer-controller -n kube-system || true
            kubectl delete deployment aws-load-balancer-controller -n kube-system || true
            kubectl delete service aws-load-balancer-webhook-service -n kube-system || true
            eksctl delete iamserviceaccount --cluster=$EKS_CLUSTER_NAME --namespace=kube-system --name=aws-load-balancer-controller || true
            sleep 30
          fi
        fi
        
        # Check if controller is working, if not install it
        if ! kubectl get deployment -n kube-system aws-load-balancer-controller > /dev/null 2>&1 || ! kubectl wait --for=condition=available deployment/aws-load-balancer-controller -n kube-system --timeout=10s; then
          echo "Installing AWS Load Balancer Controller..."
          
          # Download IAM policy
          curl -O https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/v2.5.4/docs/install/iam_policy.json
          
          # Create IAM policy (ignore if exists)
          aws iam create-policy \
            --policy-name AWSLoadBalancerControllerIAMPolicy \
            --policy-document file://iam_policy.json || echo "Policy already exists"
          
          # Create service account with IAM role
          eksctl create iamserviceaccount \
            --cluster=$EKS_CLUSTER_NAME \
            --namespace=kube-system \
            --name=aws-load-balancer-controller \
            --role-name AmazonEKSLoadBalancerControllerRole \
            --attach-policy-arn=arn:aws:iam::$ACCOUNT_ID:policy/AWSLoadBalancerControllerIAMPolicy \
            --approve
          
          # Add Helm repo and update
          helm repo add eks https://aws.github.io/eks-charts
          helm repo update
          
          # Install AWS Load Balancer Controller
          helm install aws-load-balancer-controller eks/aws-load-balancer-controller \
            -n kube-system \
            --set clusterName=$EKS_CLUSTER_NAME \
            --set serviceAccount.create=false \
            --set serviceAccount.name=aws-load-balancer-controller \
            --wait --timeout=300s
          
          echo "AWS Load Balancer Controller installed successfully"
        else
          echo "AWS Load Balancer Controller is already running and ready"
        fi

    - name: Wait for Load Balancer Controller
      run: |
        echo "Waiting for AWS Load Balancer Controller to be ready..."
        kubectl wait --for=condition=ready pod -l app.kubernetes.io/name=aws-load-balancer-controller -n kube-system --timeout=300s
        
        # Verify webhook service is available
        echo "Verifying webhook service..."
        kubectl get service aws-load-balancer-webhook-service -n kube-system
        
        # Wait a bit more for webhook to be fully ready
        sleep 30

    - name: Deploy Application
      env:
        IMAGE_URI: ${{ needs.build.outputs.image }}
      run: |
        # Update the image in deployment
        sed -i "s|ACCOUNT_ID.dkr.ecr.REGION.amazonaws.com/flask-microservice-app:latest|$IMAGE_URI|g" k8s/deployment.yaml
        
        # Apply Kubernetes manifests in order
        kubectl apply -f k8s/namespace.yaml
        
        # Deploy application first (without ingress)
        kubectl apply -f k8s/deployment.yaml
        kubectl apply -f k8s/service.yaml
        kubectl apply -f k8s/hpa.yaml
        
        # Wait for deployment to be ready
        kubectl rollout status deployment/flask-app -n flask-app --timeout=300s
        
        # Now deploy ingress after everything is ready
        kubectl apply -f k8s/ingress.yaml
        
        echo "Application deployed successfully!"

    - name: Get Application Status
      run: |
        echo "=== Deployment Status ==="
        kubectl get pods -n flask-app
        echo ""
        echo "=== Service Status ==="
        kubectl get svc -n flask-app
        echo ""
        echo "=== Ingress Status ==="
        kubectl get ingress -n flask-app
        echo ""
        echo "Waiting for load balancer to be ready..."
        sleep 60
        kubectl get ingress flask-app-ingress -n flask-app

  test:
    name: Test Application
    runs-on: ubuntu-latest
    needs: [build, deploy]
    if: github.ref == 'refs/heads/main'

    steps:
    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        role-to-assume: ${{ vars.AWS_ROLE_ARN }}
        aws-region: ${{ env.AWS_REGION }}

    - name: Update kube config
      run: aws eks update-kubeconfig --name $EKS_CLUSTER_NAME --region $AWS_REGION

    - name: Test Application Health
      run: |
        # Test via port-forward first
        echo "Testing application via port-forward..."
        kubectl port-forward -n flask-app svc/flask-app-service 8080:80 &
        sleep 10
        
        # Test health endpoint
        if curl -f http://localhost:8080/health; then
          echo "✅ Health check passed"
        else
          echo "❌ Health check failed"
          exit 1
        fi
        
        # Test main endpoint
        if curl -f http://localhost:8080/; then
          echo "✅ Main endpoint test passed"
        else
          echo "❌ Main endpoint test failed"
          exit 1
        fi
        
        # Test API endpoint
        if curl -f http://localhost:8080/api/users; then
          echo "✅ API endpoint test passed"
        else
          echo "❌ API endpoint test failed"
          exit 1
        fi
        
        # Try to get load balancer URL
        LB_URL=$(kubectl get ingress flask-app-ingress -n flask-app -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' 2>/dev/null || echo "")
        
        if [ ! -z "$LB_URL" ]; then
          echo "🌐 Application will be accessible at: http://$LB_URL"
          echo "🔒 HTTPS will be available at: https://$LB_URL"
        else
          echo "⏳ Load balancer URL not ready yet. Check later with:"
          echo "kubectl get ingress flask-app-ingress -n flask-app"
        fi